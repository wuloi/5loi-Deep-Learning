{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://5loi.com/about_loi\"><img src=\"images/DLI_Header.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 卷积神经网络\n",
    "本练习中，您将再次使用美国手语数据集训练模型。上一次我们已能对训练数据集获得很高的准确率，但模型并没有很好地泛化到验证数据集。这种无法很好地泛化到非训练数据上的行为称为*过拟合*。在本节中，我们将介绍一种流行的模型，称为[卷积神经网络](https://www.youtube.com/watch?v=x_VrgWTKkiM&vl=en)（CNN），特别适合读取图像并对其进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在完成本节时，您将能够：\n",
    "* 专门为 CNN 准备数据\n",
    "* 创建更复杂的 CNN 模型，了解多种类型的模型层\n",
    "* 训练 CNN 模型并观察其性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 加载和准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们像上一个 notebook 中那样加载 DataFrame："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/asl_data/sign_mnist_train.csv\")\n",
    "valid_df = pd.read_csv(\"data/asl_data/sign_mnist_valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASL 数据已经是展开的了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[107, 118, 127, ..., 204, 203, 202],\n",
       "       [155, 157, 156, ..., 103, 135, 149],\n",
       "       [187, 188, 188, ..., 195, 194, 195],\n",
       "       [211, 211, 212, ..., 222, 229, 163],\n",
       "       [164, 167, 170, ..., 163, 164, 179]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = train_df.head().copy()  # Grab the top 5 rows\n",
    "sample_df.pop('label')\n",
    "sample_x = sample_df.values\n",
    "sample_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这种格式下，我们没有关于哪些像素彼此相邻的所有信息。因此，我们无法应用能检测特征的卷积操作。让我们用 [reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) 对数据集进行纬度变换，使其变成 28x28 像素的格式。这将允许我们的卷积操作关联像素组并检测重要特征。\n",
    "\n",
    "注意，对于我们模型的第一个卷积层，我们不仅需要图像的高度和宽度，还需要[颜色通道](https://www.photoshopessentials.com/essentials/rgb/)的数量。我们的图像是灰度的，所以只有 1 个通道。\n",
    "\n",
    "这意味着需要将当前形状 `(5, 784)` 转换为 `(5, 1, 28, 28)`。使用 [NumPy](https://numpy.org/doc/stable/index.html) 数组，我们可以为任何我们希望保持不变的维度传递 `-1`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_HEIGHT = 28\n",
    "IMG_WIDTH = 28\n",
    "IMG_CHS = 1\n",
    "\n",
    "sample_x = sample_x.reshape(-1, IMG_CHS, IMG_HEIGHT, IMG_WIDTH)\n",
    "sample_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 创建数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们把上述步骤加到 `MyDataset` 类中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的类定义中有 4 个 `FIXME`。你能把它们替换成正确的值么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, base_df):\n",
    "        x_df = base_df.copy()  # Some operations below are in-place\n",
    "        y_df = x_df.pop(FIXME)\n",
    "        x_df = x_df.values / 255  # Normalize values from 0 to 1\n",
    "        x_df = x_df.reshape(-1, FIXME, FIXME, FIXME)\n",
    "        self.xs = torch.tensor(x_df).float().to(device)\n",
    "        self.ys = torch.tensor(y_df).to(device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.xs[idx]\n",
    "        y = self.ys[idx]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点击下方的 `...` 查看答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, base_df):\n",
    "        x_df = base_df.copy()  # Some operations below are in-place\n",
    "        y_df = x_df.pop('label')\n",
    "        x_df = x_df.values / 255  # Normalize values from 0 to 1\n",
    "        x_df = x_df.reshape(-1, IMG_CHS, IMG_WIDTH, IMG_HEIGHT)\n",
    "        self.xs = torch.tensor(x_df).float().to(device)\n",
    "        self.ys = torch.tensor(y_df).to(device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.xs[idx]\n",
    "        y = self.ys[idx]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 创建 DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，让我们从 Dataset 中创建 DataLoader。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的其中一个函数调用里少了一个 `shuffle=True` 参数。你能回忆起是哪个并把它加上去么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data = MyDataset(train_df)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "train_N = len(train_loader.dataset)\n",
    "\n",
    "valid_data = MyDataset(valid_df)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "valid_N = len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点击下方的 `...` 查看答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面从 DataLoader 中取出一个批次来确认数据格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.5020, 0.5255, 0.5490,  ..., 0.6627, 0.6510, 0.6471],\n",
       "           [0.5176, 0.5412, 0.5608,  ..., 0.6784, 0.6706, 0.6667],\n",
       "           [0.5255, 0.5490, 0.5647,  ..., 0.6902, 0.6863, 0.6824],\n",
       "           ...,\n",
       "           [0.3569, 0.3647, 0.3765,  ..., 0.7647, 0.7412, 0.7373],\n",
       "           [0.3647, 0.3647, 0.3686,  ..., 0.3725, 0.5922, 0.8392],\n",
       "           [0.3686, 0.3686, 0.3765,  ..., 0.4902, 0.6314, 0.7490]]],\n",
       " \n",
       " \n",
       "         [[[0.6667, 0.6863, 0.7059,  ..., 0.7373, 0.5333, 0.5176],\n",
       "           [0.6824, 0.7020, 0.7216,  ..., 0.7333, 0.6902, 0.4510],\n",
       "           [0.6980, 0.7255, 0.7412,  ..., 0.7373, 0.7569, 0.6039],\n",
       "           ...,\n",
       "           [0.4706, 0.4706, 0.4745,  ..., 0.1098, 0.0627, 0.0275],\n",
       "           [0.4784, 0.4784, 0.4824,  ..., 0.0863, 0.0824, 0.0235],\n",
       "           [0.4784, 0.4745, 0.5020,  ..., 0.0627, 0.0863, 0.0510]]],\n",
       " \n",
       " \n",
       "         [[[0.4745, 0.4863, 0.5059,  ..., 0.5451, 0.5412, 0.5294],\n",
       "           [0.4784, 0.4941, 0.5137,  ..., 0.5529, 0.5451, 0.5451],\n",
       "           [0.4824, 0.5020, 0.5216,  ..., 0.5569, 0.5608, 0.5176],\n",
       "           ...,\n",
       "           [0.3216, 0.3176, 0.3137,  ..., 0.2706, 0.2667, 0.2706],\n",
       "           [0.3333, 0.3333, 0.3373,  ..., 0.2824, 0.2706, 0.2588],\n",
       "           [0.3412, 0.3373, 0.3451,  ..., 0.3059, 0.2157, 0.1412]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.3804, 0.3961, 0.4000,  ..., 0.5137, 0.5137, 0.5176],\n",
       "           [0.3882, 0.3961, 0.4000,  ..., 0.5216, 0.5176, 0.5216],\n",
       "           [0.3922, 0.4000, 0.4078,  ..., 0.5255, 0.5255, 0.5255],\n",
       "           ...,\n",
       "           [0.4549, 0.4588, 0.4745,  ..., 0.6039, 0.6039, 0.6000],\n",
       "           [0.4588, 0.4627, 0.4784,  ..., 0.6078, 0.6078, 0.6039],\n",
       "           [0.4588, 0.4627, 0.4824,  ..., 0.6118, 0.6078, 0.6078]]],\n",
       " \n",
       " \n",
       "         [[[0.7098, 0.7176, 0.7255,  ..., 0.7922, 0.7922, 0.7922],\n",
       "           [0.7176, 0.7216, 0.7294,  ..., 0.7961, 0.7922, 0.7961],\n",
       "           [0.7216, 0.7255, 0.7373,  ..., 0.8000, 0.8000, 0.7961],\n",
       "           ...,\n",
       "           [0.7725, 0.7725, 0.7804,  ..., 0.8510, 0.8510, 0.8510],\n",
       "           [0.7686, 0.7725, 0.7804,  ..., 0.8471, 0.8471, 0.8471],\n",
       "           [0.7686, 0.7765, 0.7843,  ..., 0.8471, 0.8471, 0.8431]]],\n",
       " \n",
       " \n",
       "         [[[0.6078, 0.6157, 0.6392,  ..., 0.7686, 0.7608, 0.7569],\n",
       "           [0.6196, 0.6275, 0.6471,  ..., 0.7725, 0.7725, 0.7647],\n",
       "           [0.6275, 0.6392, 0.6588,  ..., 0.7843, 0.7843, 0.7804],\n",
       "           ...,\n",
       "           [0.8196, 0.8392, 0.8706,  ..., 0.2941, 0.2941, 0.3608],\n",
       "           [0.8196, 0.8471, 0.8745,  ..., 0.2745, 0.2667, 0.3412],\n",
       "           [0.8196, 0.8431, 0.8745,  ..., 0.3059, 0.2353, 0.2745]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([21, 22,  1, 23, 12, 16, 16,  8, 22,  7,  5, 22, 14,  8, 18,  1,  1,  8,\n",
       "          7, 20,  8, 17, 23, 14, 16,  7, 21,  2, 16, 13, 11,  4],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它看起来有变化了，再通过 `shape` 确认一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 创建卷积模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如今，许多数据科学家通过借鉴类似项目的模型配置来开始他们的项目。假设问题并非毫不相关，那么很有可能已经有人创建了表现很好的模型，并将其发布在 [TensorFlow Hub](https://www.tensorflow.org/hub) 和 [NGC Catalog](https://ngc.nvidia.com/catalog/models) 等在线仓库中。今天，我们将提供一个适用于这个问题的模型。\n",
    "\n",
    "<img src=\"images/cnn.png\" width=180 />\n",
    "\n",
    "我们在讲座中介绍了许多不同类型的层，这里我们将逐一回顾它们，并提供它们的文档链接。如有疑问，请阅读官方文档（或在 [Stack Overflow](https://stackoverflow.com/) 上提问）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 24\n",
    "kernel_size = 3\n",
    "flattened_img_size = 75 * 3 * 3\n",
    "\n",
    "model = nn.Sequential(\n",
    "    # First convolution\n",
    "    nn.Conv2d(IMG_CHS, 25, kernel_size, stride=1, padding=1),  # 25 x 28 x 28\n",
    "    nn.BatchNorm2d(25),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, stride=2),  # 25 x 14 x 14\n",
    "    # Second convolution\n",
    "    nn.Conv2d(25, 50, kernel_size, stride=1, padding=1),  # 50 x 14 x 14\n",
    "    nn.BatchNorm2d(50),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(.2),\n",
    "    nn.MaxPool2d(2, stride=2),  # 50 x 7 x 7\n",
    "    # Third convolution\n",
    "    nn.Conv2d(50, 75, kernel_size, stride=1, padding=1),  # 75 x 7 x 7\n",
    "    nn.BatchNorm2d(75),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, stride=2),  # 75 x 3 x 3\n",
    "    # Flatten to Dense\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(flattened_img_size, 512),\n",
    "    nn.Dropout(.3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, n_classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 [Conv2D](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/conv2d.png\" width=300 />\n",
    "\n",
    "这些是我们的 2D 卷积层。小型卷积核（kernel）将在输入图像上滑动，检测对分类重要的特征。模型早期的卷积将检测简单的特征，如线条。后面的卷积将检测更复杂的特征。让我们看看第一个 Conv2D 层:\n",
    "```Python\n",
    "nn.Conv2d(IMG_CHS, 25, kernel_size, stride=1, padding=1)\n",
    "```\n",
    "25 指的是将要学习的滤波器（filter）数量。尽管 `kernel_size = 3`，PyTorch 会假设我们想要的是 3 x 3 的滤波器。`stride` 指的是滤波器在图像上滑动时的步长。`padding` 决定了由滤波器创建的输出图像是否与输入图像的大小匹配。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 [BatchNormalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与归一化输入类似，批量归一化通过缩放隐藏层中的值来改善训练。[在这里]((https://blog.paperspace.com/busting-the-myths-about-batch-normalization/))可以阅读更多详细信息。\n",
    "\n",
    "关于批量归一化层应该放在哪里最好，存在一些争议。[这个 Stack Overflow 帖子](https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout)汇集了许多观点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 [MaxPool2D](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/maxpool2d.png\" width=300 />\n",
    "\n",
    "最大池化实质上是将图像缩小到较低的分辨率。这样做是为了帮助模型对平移（translation，物体左右移动）具有鲁棒性，同时也使我们的模型运行更快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dropout.png\" width=360 />\n",
    "\n",
    "Dropout 是一种防止过拟合的技术。Dropout 随机选择一部分神经元并将其关闭，使它们在特定的前向或后向传播中不参与计算。这有助于确保网络具有鲁棒性和冗余性，不会过度依赖任何一个区域来得出结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 [Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten 层将一个多维的层输出展平成一维数组。这个输出被称为特征向量，将连接到最终的分类层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.6 [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在早期的模型中已经见过全连接的线性层。我们的第一个全连接层（512个单元）以特征向量为输入，学习哪些特征将对特定分类有贡献。第二个全连接层（24个单元）是最终的分类层，输出我们的预测结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 总结模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可能感觉信息量有点大，但不用担心。现在不需要完全理解所有内容就能有效地训练卷积模型。最重要的是我们知道了它可以帮助从图像中提取有用信息，并可以用于分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): Sequential(\n",
       "    (0): Conv2d(1, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(25, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(50, 75, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (13): Flatten(start_dim=1, end_dim=-1)\n",
       "    (14): Linear(in_features=675, out_features=512, bias=True)\n",
       "    (15): Dropout(p=0.3, inplace=False)\n",
       "    (16): ReLU()\n",
       "    (17): Linear(in_features=512, out_features=24, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.compile(model.to(device))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们试图解决的问题仍然相同（分类ASL图像），我们将继续使用相同的`损失函数`和`准确率`指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_accuracy(output, y, N):\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "    return correct / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管模型架构差别很大，但训练过程看上去完全一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些是与之前相同的 `train` 和 `validate` 函数，但它们被混在一起了。你能正确命名每个函数并替换 `FIXME` 吗？"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def FIXME():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.FIXME()\n",
    "    with torch.no_grad():\n",
    "        for x, y in FIXME:\n",
    "            output = model(x)\n",
    "\n",
    "            loss += loss_function(output, y).item()\n",
    "            accuracy += get_batch_accuracy(output, y, valid_N)\n",
    "    print('FIXME - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def FIXME():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.FIXME()\n",
    "    for x, y in FIXME:\n",
    "        output = model(x)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = loss_function(output, y)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += batch_loss.item()\n",
    "        accuracy += get_batch_accuracy(output, y, train_N)\n",
    "    print('FIXME - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点击下方的 `...` 查看答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def validate():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loader:\n",
    "            output = model(x)\n",
    "\n",
    "            loss += loss_function(output, y).item()\n",
    "            accuracy += get_batch_accuracy(output, y, valid_N)\n",
    "    print('Valid - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def train():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        output = model(x)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = loss_function(output, y)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += batch_loss.item()\n",
    "        accuracy += get_batch_accuracy(output, y, train_N)\n",
    "    print('Train - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 263.0673 Accuracy: 0.9101\n",
      "Valid - Loss: 25.3139 Accuracy: 0.9564\n",
      "Epoch: 1\n",
      "Train - Loss: 14.8527 Accuracy: 0.9958\n",
      "Valid - Loss: 14.5266 Accuracy: 0.9774\n",
      "Epoch: 2\n",
      "Train - Loss: 15.0548 Accuracy: 0.9949\n",
      "Valid - Loss: 18.2880 Accuracy: 0.9736\n",
      "Epoch: 3\n",
      "Train - Loss: 6.3722 Accuracy: 0.9981\n",
      "Valid - Loss: 6.9258 Accuracy: 0.9922\n",
      "Epoch: 4\n",
      "Train - Loss: 13.2297 Accuracy: 0.9958\n",
      "Valid - Loss: 18.2248 Accuracy: 0.9791\n",
      "Epoch: 5\n",
      "Train - Loss: 0.5619 Accuracy: 0.9999\n",
      "Valid - Loss: 10.3425 Accuracy: 0.9824\n",
      "Epoch: 6\n",
      "Train - Loss: 15.8552 Accuracy: 0.9952\n",
      "Valid - Loss: 14.1589 Accuracy: 0.9819\n",
      "Epoch: 7\n",
      "Train - Loss: 7.1374 Accuracy: 0.9973\n",
      "Valid - Loss: 22.8486 Accuracy: 0.9636\n",
      "Epoch: 8\n",
      "Train - Loss: 2.6616 Accuracy: 0.9990\n",
      "Valid - Loss: 23.7614 Accuracy: 0.9664\n",
      "Epoch: 9\n",
      "Train - Loss: 9.1858 Accuracy: 0.9973\n",
      "Valid - Loss: 14.9869 Accuracy: 0.9721\n",
      "Epoch: 10\n",
      "Train - Loss: 1.5486 Accuracy: 0.9994\n",
      "Valid - Loss: 16.3305 Accuracy: 0.9770\n",
      "Epoch: 11\n",
      "Train - Loss: 8.4807 Accuracy: 0.9976\n",
      "Valid - Loss: 27.0920 Accuracy: 0.9619\n",
      "Epoch: 12\n",
      "Train - Loss: 1.3113 Accuracy: 0.9996\n",
      "Valid - Loss: 11.9306 Accuracy: 0.9820\n",
      "Epoch: 13\n",
      "Train - Loss: 1.1144 Accuracy: 0.9999\n",
      "Valid - Loss: 13.6323 Accuracy: 0.9802\n",
      "Epoch: 14\n",
      "Train - Loss: 11.1582 Accuracy: 0.9961\n",
      "Valid - Loss: 11.2427 Accuracy: 0.9824\n",
      "Epoch: 15\n",
      "Train - Loss: 0.3896 Accuracy: 0.9999\n",
      "Valid - Loss: 11.6703 Accuracy: 0.9803\n",
      "Epoch: 16\n",
      "Train - Loss: 0.1416 Accuracy: 1.0000\n",
      "Valid - Loss: 6.1168 Accuracy: 0.9895\n",
      "Epoch: 17\n",
      "Train - Loss: 0.0442 Accuracy: 1.0000\n",
      "Valid - Loss: 8.5839 Accuracy: 0.9851\n",
      "Epoch: 18\n",
      "Train - Loss: 11.2061 Accuracy: 0.9965\n",
      "Valid - Loss: 25.5757 Accuracy: 0.9649\n",
      "Epoch: 19\n",
      "Train - Loss: 1.4569 Accuracy: 0.9994\n",
      "Valid - Loss: 15.7891 Accuracy: 0.9736\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    train()\n",
    "    validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 结果讨论\n",
    "看起来大有改善！训练准确率非常高，且验证准确率也已得到提升。这是一个很棒的结果，因为我们所做的就是换了一个新模型。\n",
    "\n",
    "您可能还会看到验证准确率有所波动，这表明我们的模型的泛化能力还有改善余地。好在我们还有别的措施供我们使用，下一讲中我们继续讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本节中，您利用了几种新的层来实现 CNN，其表现比上一节中使用的简单的模型更好。希望您对使用准备好的数据创建和训练模型的整个过程更加熟悉。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 清理显存\n",
    "继续后面的内容前，请执行以下单元清理 GPU 显存。转至下一 notebook 之前需要执行此操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 下一步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前面的几节中，您专注于模型的创建和训练。为了进一步提高性能，您的注意力将转移到*数据增强*，这是一组技术的集合，可以使您的模型在比原来更多更好的可用数据上进行训练。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
