{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fcfa71",
   "metadata": {},
   "source": [
    "<center><a href=\"https://5loi.com/about_loi\"><img src=\"images/DLI_Header.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6704e1",
   "metadata": {},
   "source": [
    "# 6. 自然语言处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9d3cf",
   "metadata": {},
   "source": [
    "在这个教程中，我们将离开像静态图像这样的独立数据，转向依赖于序列中其它数据项的数据。作为示例，我们将使用句子文本。语言自然由序列数据组成，形式为单词中的字符和句子中的单词。其它序列数据的例子包括股票价格和天气数据等时间序列。视频虽然包含静态图像，但也属于序列。当数据中的元素与之前和之后的内容有关系，我们就需要采取不同的方法来处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c166f5",
   "metadata": {},
   "source": [
    "## 6.1 目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b2afe7",
   "metadata": {},
   "source": [
    "* 使用分词器为神经网络准备文本\n",
    "* 观察如何使用嵌入来识别文本数据的数值特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896302a8",
   "metadata": {},
   "source": [
    "## 6.2 BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed512137",
   "metadata": {},
   "source": [
    "BERT，全称为 **B**idirectional **E**ncoder **R**epresentations from **T**ransformers，是 [Google](https://www.google.com/) 在 2018 年引入的一种开创性模型。\n",
    "\n",
    "BERT 同时针对两个目标进行训练：\n",
    "* 从一系列单词中预测缺失的单词\n",
    "* 在一系列句子之后预测新的句子\n",
    "\n",
    "让我们用这两种挑战来看看 BERT 的表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4571fbb7",
   "metadata": {},
   "source": [
    "## 6.3 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76984d1a",
   "metadata": {},
   "source": [
    "由于神经网络是数值计算机，让我们将文本转换为数值 token。让我们加载 BERT 的[分词器](https://huggingface.co/docs/transformers/main_classes/tokenizer#tokenizer)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c2d2352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a221eb85",
   "metadata": {},
   "source": [
    "BERT 的`分词器`可以一次性[编码](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode)多段文本。稍后我们将测试 BERT 的记忆力，先给它一些信息和一个关于信息的问题。您可以随时回到这里，尝试不同的句子组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0b18f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 146,\n",
       " 2437,\n",
       " 11838,\n",
       " 117,\n",
       " 1241,\n",
       " 1103,\n",
       " 3014,\n",
       " 1105,\n",
       " 186,\n",
       " 18413,\n",
       " 21961,\n",
       " 1348,\n",
       " 119,\n",
       " 102,\n",
       " 1327,\n",
       " 1912,\n",
       " 1104,\n",
       " 11838,\n",
       " 1202,\n",
       " 146,\n",
       " 2437,\n",
       " 136,\n",
       " 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_1 = \"I understand equations, both the simple and quadratical.\"\n",
    "text_2 = \"What kind of equations do I understand?\"\n",
    "\n",
    "# Tokenized input with special tokens around it (for BERT: [CLS] at the beginning and [SEP] at the end)\n",
    "indexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)\n",
    "indexed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390f931f",
   "metadata": {},
   "source": [
    "如果我们计算 token 的数量，会发现句子中的 token 数量多于单词数。让我们来看看为什么会这样。可以用 [convert_ids_to_tokens](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.convert_ids_to_tokens) 来查看使用了哪些 token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5435fca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'I',\n",
       " 'understand',\n",
       " 'equations',\n",
       " ',',\n",
       " 'both',\n",
       " 'the',\n",
       " 'simple',\n",
       " 'and',\n",
       " 'q',\n",
       " '##uad',\n",
       " '##ratic',\n",
       " '##al',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'What',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'equations',\n",
       " 'do',\n",
       " 'I',\n",
       " 'understand',\n",
       " '?',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([str(token) for token in indexed_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb501dc",
   "metadata": {},
   "source": [
    "索引列表比原始输入长的原因有两个：\n",
    "1. `tokenizer` 添加了 `special_tokens` 来表示序列的开始（`[CLS]`）和句子之间的分隔（`[SEP]`）。\n",
    "2. `tokenizer` 可以将一个词分解成多个部分。\n",
    "\n",
    "从语言学角度来看，第二点很有趣。许多语言都有[词根](https://zh.wikipedia.org/wiki/%E8%AF%8D%E6%A0%B9)，或构成单词的组成部分。例如，“quadratic”这个词的词根是“quadr”，意思是“4”。BERT 并不是用语言定义词根，而是使用 [WordPiece](https://paperswithcode.com/method/wordpiece) 模型来找出如何拆分单词模式。我们今天使用的 BERT 模型有一个 `28996` 个 token 的词汇表。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc9975",
   "metadata": {},
   "source": [
    "我们可以直接[解码](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)编码过的文本。注意 `special_tokens` 已经被添加进去了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1498e60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] I understand equations, both the simple and quadratical. [SEP] What kind of equations do I understand? [SEP]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(indexed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4020bac7",
   "metadata": {},
   "source": [
    "## 6.4 文本分段"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48df212",
   "metadata": {},
   "source": [
    "为了使用 BERT 模型进行预测，它还需要一个 `segment_ids` 的列表。这是一个与我们 token 相同长度的向量，表示每个句子属于哪个段落。\n",
    "\n",
    "由于我们的 `tokenizer` 添加了一些 `special_tokens`，我们可以使用这些特殊标记来找到段落。首先，让我们定义哪个索引对应哪个特殊标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e71529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token = 101\n",
    "sep_token = 102"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0cc070",
   "metadata": {},
   "source": [
    "接下来，我们可以创建一个 `for` 循环。我们将从 `segment_id` 设置为 `0` 开始，并且每当我们看到 [SEP] 标记时就增加 `segment_id`。为了确保效果，我们将在稍后将这些 `segment_ids` 和 `indexed_tokens` 作为张量输入模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feace8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_ids(indexed_tokens):\n",
    "    segment_ids = []\n",
    "    segment_id = 0\n",
    "    for token in indexed_tokens:\n",
    "        if token == sep_token:\n",
    "            segment_id += 1\n",
    "        segment_ids.append(segment_id)\n",
    "    segment_ids[-1] -= 1  # Last [SEP] is ignored\n",
    "    return torch.tensor([segment_ids]), torch.tensor([indexed_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebae44e",
   "metadata": {},
   "source": [
    "让我们测试一下。每个数字是否正确地对应第一句和第二句？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4e062f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments_tensors, tokens_tensor = get_segment_ids(indexed_tokens)\n",
    "segments_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1233f43",
   "metadata": {},
   "source": [
    "## 6.4 文本掩码（Text Masking）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d81d87",
   "metadata": {},
   "source": [
    "让我们先看看 BERT 对单词的处理。为了训练词嵌入，BERT 在一系列单词中掩掉一个单词。掩码用了一个特殊的标记："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65dfb96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9792811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b50ad",
   "metadata": {},
   "source": [
    "让我们从之前的两个句子中选择位置索引 `5` 进行掩码。随时回到这里改变索引，看看结果会如何变化！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b77f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_index = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8932dbda",
   "metadata": {},
   "source": [
    "接下来，我们将应用掩码并验证它是否出现在句子序列中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6deac9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] I understand equations, [MASK] the simple and quadratical. [SEP] What kind of equations do I understand? [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_tokens[masked_index] = tokenizer.mask_token_id\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "tokenizer.decode(indexed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6aea80",
   "metadata": {},
   "source": [
    "然后，我们将加载用于预测缺失单词的模型：`modelForMaskedLM`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "908dd216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba64f0953fd4d7d9df3c8c205390002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='model.safetensors', max=435755784.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "masked_lm_model = torch.hub.load('huggingface/pytorch-transformers', 'modelForMaskedLM', 'bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b606a7b",
   "metadata": {},
   "source": [
    "就像使用其它 PyTorch 模块一样，我们可以检查其架构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78313cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_lm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00505af9",
   "metadata": {},
   "source": [
    "您能找到标有 `word_embeddings` 的部分吗？这些是 BERT 为每个 token 学习到的嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "349ff0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0005, -0.0416,  0.0131,  ..., -0.0039, -0.0335,  0.0150],\n",
       "        [ 0.0169, -0.0311,  0.0042,  ..., -0.0147, -0.0356, -0.0036],\n",
       "        [-0.0006, -0.0267,  0.0080,  ..., -0.0100, -0.0331, -0.0165],\n",
       "        ...,\n",
       "        [-0.0064,  0.0166, -0.0204,  ..., -0.0418, -0.0492,  0.0042],\n",
       "        [-0.0048, -0.0027, -0.0290,  ..., -0.0512,  0.0045, -0.0118],\n",
       "        [ 0.0313, -0.0297, -0.0230,  ..., -0.0145, -0.0525,  0.0284]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_table = next(masked_lm_model.bert.embeddings.word_embeddings.parameters())\n",
    "embedding_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c8ad3",
   "metadata": {},
   "source": [
    "我们可以验证 BERT 词汇表中的 `28996` 个 token 都有一个大小为 `768` 的嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b9ba827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28996, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020544fd",
   "metadata": {},
   "source": [
    "让我们测试一下模型！它能正确预测我们提供的句子中缺失的单词吗？我们将使用 [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html) 来让 PyTorch 不计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fef11d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ -7.3832,  -7.2504,  -7.4539,  ...,  -6.0597,  -5.7928,  -6.2133],\n",
       "         [ -6.7681,  -6.7896,  -6.8317,  ...,  -5.4655,  -5.4048,  -6.0683],\n",
       "         [ -7.7323,  -7.9597,  -7.7348,  ...,  -5.7611,  -5.3566,  -4.3361],\n",
       "         ...,\n",
       "         [ -6.1213,  -6.3311,  -6.4144,  ...,  -5.8884,  -4.1157,  -3.1189],\n",
       "         [-12.3216, -12.4479, -11.9787,  ..., -10.6539,  -8.7396, -11.0487],\n",
       "         [-13.4115, -13.7876, -13.5183,  ..., -10.6359, -11.6582, -10.9009]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = masked_lm_model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce11d45",
   "metadata": {},
   "source": [
    "这有点难读懂，让我们看一下 `shape`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae4efa98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 28996])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc56db7",
   "metadata": {},
   "source": [
    "`24` 指的是是 token 数，`28996` 是指对 BERT 词汇表中每个 token 的预测。我们想找到词汇表中所有 token 的最大值，可以用 [torch.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "803815c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1241"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the predicted token\n",
    "predicted_index = torch.argmax(predictions[0][0], dim=1)[masked_index].item()\n",
    "predicted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8f7da",
   "metadata": {},
   "source": [
    "让我们看看 token `1241` 对应的是什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dc45267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'both'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "predicted_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066afef9",
   "metadata": {},
   "source": [
    "您觉得怎么样？正确吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23a1eb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] I understand equations, [MASK] the simple and quadratical. [SEP] What kind of equations do I understand? [SEP]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(indexed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56a36e",
   "metadata": {},
   "source": [
    "## 6.5 问题与回答"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83910deb",
   "metadata": {},
   "source": [
    "虽然词掩码很有趣，但 BERT 是为更复杂的问题设计的，比如句子预测。它能通过 [Attention Transformer](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) 架构来完成这一任务。\n",
    "\n",
    "在这一部分，我们将使用 BERT 的不同版本，它有自己的 tokenizer。让我们为我们的示例句子找到一组新的 token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f45877e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633624ee074c45fd910eb9640aa7883b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='tokenizer_config.json', max=48.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cddaa82cd3248d9941944b7c56ce28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='config.json', max=443.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472d5f33c1664fa9b4ec5b71ef2af34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='vocab.txt', max=231508.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e33fb4053af480585a5b1d2046677ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='tokenizer.json', max=466062.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"I understand equations, both the simple and quadratical.\"\n",
    "text_2 = \"What kind of equations do I understand?\"\n",
    "\n",
    "question_answering_tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "indexed_tokens = question_answering_tokenizer.encode(text_1, text_2, add_special_tokens=True)\n",
    "segments_tensors, tokens_tensor = get_segment_ids(indexed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a87b3",
   "metadata": {},
   "source": [
    "接下来，让我们加载 `question_answering_model`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12d9e415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a772851e89d547a99d5145d42029c60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='model.safetensors', max=1340622760.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "question_answering_model = torch.hub.load('huggingface/pytorch-transformers', 'modelForQuestionAnswering', 'bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1695c7d",
   "metadata": {},
   "source": [
    "我们可以像在掩掉单词时一样输入 tokens 和 segments。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f2ce72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-5.5943, -4.2960, -5.2682, -1.2511, -6.8350, -0.3992,  2.2274,  2.4654,\n",
       "         -6.6066,  2.5014, -4.4613, -4.8040, -7.8383, -5.5944, -4.7833, -6.9730,\n",
       "         -7.1477, -5.2967, -7.4825, -6.7737, -6.8806, -8.6612, -5.5944]]), end_logits=tensor([[-0.7409, -5.3478, -4.2317, -0.0275, -2.6293, -5.9589, -2.8828,  2.7770,\n",
       "         -4.8512, -2.2092, -2.2413,  4.4412, -0.7181, -0.7411, -3.8988, -5.3865,\n",
       "         -5.0452, -4.4974, -6.3098, -5.5938, -5.5562, -5.3034, -0.7412]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the start and end positions logits\n",
    "with torch.no_grad():\n",
    "    out = question_answering_model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e19220",
   "metadata": {},
   "source": [
    "`question_answering_model` 和问答模型正在扫描我们的输入序列，以找到最能回答问题的子序列。数值越高，答案就越有可能是从这里开始的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e778f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.5943, -4.2960, -5.2682, -1.2511, -6.8350, -0.3992,  2.2274,  2.4654,\n",
       "         -6.6066,  2.5014, -4.4613, -4.8040, -7.8383, -5.5944, -4.7833, -6.9730,\n",
       "         -7.1477, -5.2967, -7.4825, -6.7737, -6.8806, -8.6612, -5.5944]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.start_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee8e771",
   "metadata": {},
   "source": [
    "同样，`end_logits` 中的数越高，答案就越可能结束在那个 token 上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa1db4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7409, -5.3478, -4.2317, -0.0275, -2.6293, -5.9589, -2.8828,  2.7770,\n",
       "         -4.8512, -2.2092, -2.2413,  4.4412, -0.7181, -0.7411, -3.8988, -5.3865,\n",
       "         -5.0452, -4.4974, -6.3098, -5.5938, -5.5562, -5.3034, -0.7412]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e21de1",
   "metadata": {},
   "source": [
    "然后我们可以用 [torch.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html) 来找到从开始到结束的 `answer_sequence`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d391ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17718, 23671, 2389]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_sequence = indexed_tokens[torch.argmax(out.start_logits):torch.argmax(out.end_logits)+1]\n",
    "answer_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b0ae9",
   "metadata": {},
   "source": [
    "最后，让我们[解码](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)这些 token，看看答案是否正确！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c9b95c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quad', '##ratic', '##al']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answering_tokenizer.convert_ids_to_tokens(answer_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd7287ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quadratical'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answering_tokenizer.decode(answer_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbc259",
   "metadata": {},
   "source": [
    "## 6.7 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1545df",
   "metadata": {},
   "source": [
    "干得好！您成功地使用了大语言模型 (LLM) 从一系列句子中提取答案。尽管 BERT 在首次发布时是最先进的，但许多其它 LLM 自那以后也取得了突破。[build.nvidia.com](https://build.nvidia.com/explore/discover) 上托管了许多这样的模型，可以在浏览器中与它们互动。去探索一下，了解了解当今的最先进技术吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08d9e5",
   "metadata": {},
   "source": [
    "### 6.7.1 清理内存\n",
    "继续进行后续内容之前，请执行以下单元清理 GPU 显存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d956c98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9752608",
   "metadata": {},
   "source": [
    "### 6.7.2 下一步"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2d0ce",
   "metadata": {},
   "source": [
    "恭喜您，您已经完成了所有课程学习目标！\n",
    "\n",
    "作为最后的练习，请在评估中成功完成一个端到端的图像分类问题，完成后您将获得本课程的证书。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
