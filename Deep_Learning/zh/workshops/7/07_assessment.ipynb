{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估\n",
    "恭喜您完成了今天的课程！希望您在此过程中学到了一些有价值的技能。现在该测试一下这些技能了。在此评估中，您将训练一种能够识别新鲜和腐烂水果的新模型。您需要使模型的验证准确率达到92％，才能通过评估，但我们鼓励您挑战更高的准确率。为此，您将使用先前练习中学到的技能，具体来说，我们建议您结合使用迁移学习、数据扩充和模型微调。训练好模型并在测试数据集上的准确率达到至少92％之后，请保存模型，然后评估其准确率。让我们开始吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集\n",
    "在本练习中，您将训练一个模型来识别新鲜和腐烂的水果，数据集来自[Kaggle](https://www.kaggle.com/sriramr/fruits-fresh-and-rotten-for-classification)。如果您有兴趣在课后自己开始一个新的项目，那么Kaggle是一个值得访问的好地方。现在您可详细查看`data/fruits`文件夹中的数据集结构。水果有六类：新鲜的苹果，新鲜的橙子，新鲜的香蕉，烂的苹果，烂的橙子和烂的香蕉。这意味着您的模型将需要有6个神经元的输出层才能成功进行分类 您还需要使用`categorical_crossentropy`作为损失函数来编译模型，因为我们有两个以上的类别。\n",
    "\n",
    "<img src=\"./images/fruits.png\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载ImageNet预训练的基础模型\n",
    "我们鼓励您从在ImageNet上预训练的模型开始。您需要用正确的权重加载模型，设置输入的形状，然后选择删除模型的最后一层。请记住，图像具有三个维度：高度和宽度以及多个颜色通道。因为这些图片是彩色的，所以会有红色，绿色和蓝色三个通道。我们已经为您填写了输入形状，请不要更改，否则评估将失败。如果您需要预训练模型的参考设置，请查看笔记本05b，您在那里最先实现的迁移学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "\n",
    "# base_model = keras.applications.VGG16(\n",
    "#     weights=FIXME,\n",
    "#     input_shape=(224, 224, 3),\n",
    "#     include_top=FIXME)\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "base_model = keras.applications.VGG16(\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 冻结基础模型\n",
    "接下来，我们建议您像在笔记本05b中一样冻结基础模型。这样做是为了使从ImageNet数据集中所学到的知识都不会在初始的训练中被破坏。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "# base_model.trainable = FIXME\n",
    "\n",
    "base_model.trainable = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向模型添加新层\n",
    "现在该向预训练模型中添加新层了。您可以再次使用笔记本05b作为指导。请密切注意最后的全连接（Dense）层，并确保其具有正确数量的神经元以对不同类型的水果进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inputs with correct shape\n",
    "# inputs = FIXME\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "# Add pooling layer or flatten layer\n",
    "# x = FIXME\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add final dense layer\n",
    "# outputs = keras.layers.Dense(FIXME, activation = 'softmax')(x)\n",
    "outputs = keras.layers.Dense(1, activation = 'softmax')(x)\n",
    "\n",
    "# Combine inputs and outputs to create model\n",
    "# model = keras.Model(FIXME)\n",
    "model = keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 14,715,201\n",
      "Trainable params: 513\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编译模型\n",
    "现在可以使用损失函数（loss）和衡量标准（metrics）选项来编译模型了。请记住，我们正在训练的模型是要解决多分类而不是二分类的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss = FIXME , metrics = FIXME)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扩充数据\n",
    "如果需要，请尝试扩充数据以改进数据集。请参考笔记本04a和笔记本05b中的数据扩充的示例。您也可以查看[Keras ImageDataGenerator类](https://keras.io/api/preprocessing/image/#imagedatagenerator-class)的文档。 此步骤是可选的，但是您可能会发现，这对训练时能达到95％的准确率很有帮助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# datagen = ImageDataGenerator(FIXME)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images horizontally\n",
    "        vertical_flip=False)  # Don't randomly flip images vertically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集\n",
    "现在应该加载训练和测试数据集了。您必须选择正确的文件夹以及图像的正确的`target_size`（它必须与您创建的模型的输入高度和宽度相匹配）。如果您需要参考，可以查看笔记本05b。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1182 images belonging to 6 classes.\n",
      "Found 329 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# # load and iterate training dataset\n",
    "# train_it = datagen.flow_from_directory(FIXME, \n",
    "#                                        target_size=FIXME, \n",
    "#                                        color_mode='rgb', \n",
    "#                                        class_mode=\"categorical\")\n",
    "# # load and iterate validation dataset\n",
    "# valid_it = datagen.flow_from_directory(FIXME, \n",
    "#                                       target_size=FIXME, \n",
    "#                                       color_mode='rgb', \n",
    "#                                       class_mode=\"categorical\")\n",
    "\n",
    "# load and iterate training dataset\n",
    "train_it = datagen.flow_from_directory('data/fruits/train/', \n",
    "                                       target_size=(224, 224), \n",
    "                                       color_mode='rgb', \n",
    "                                       class_mode=\"categorical\")\n",
    "# load and iterate validation dataset\n",
    "valid_it = datagen.flow_from_directory('data/fruits/valid/', \n",
    "                                      target_size=(224, 224), \n",
    "                                      color_mode='rgb', \n",
    "                                      class_mode=\"categorical\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "现在开始训练模型！将训练和测试数据集传递给`fit`函数，并设置所需的训练次数（epochs）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "37/36 [==============================] - 21s 557ms/step - loss: 0.0535 - accuracy: 0.9935 - val_loss: 0.1354 - val_accuracy: 0.9873\n",
      "Epoch 2/20\n",
      "37/36 [==============================] - 20s 542ms/step - loss: 0.0354 - accuracy: 0.9951 - val_loss: 0.1689 - val_accuracy: 0.9848\n",
      "Epoch 3/20\n",
      "37/36 [==============================] - 20s 547ms/step - loss: 0.0394 - accuracy: 0.9952 - val_loss: 0.1329 - val_accuracy: 0.9878\n",
      "Epoch 4/20\n",
      "37/36 [==============================] - 20s 546ms/step - loss: 0.0246 - accuracy: 0.9975 - val_loss: 0.1670 - val_accuracy: 0.9889\n",
      "Epoch 5/20\n",
      "37/36 [==============================] - 20s 551ms/step - loss: 0.0253 - accuracy: 0.9970 - val_loss: 0.1263 - val_accuracy: 0.9899\n",
      "Epoch 6/20\n",
      "37/36 [==============================] - 20s 549ms/step - loss: 0.0163 - accuracy: 0.9987 - val_loss: 0.1110 - val_accuracy: 0.9899\n",
      "Epoch 7/20\n",
      "37/36 [==============================] - 20s 549ms/step - loss: 0.0155 - accuracy: 0.9989 - val_loss: 0.1065 - val_accuracy: 0.9873\n",
      "Epoch 8/20\n",
      "37/36 [==============================] - 20s 543ms/step - loss: 0.0165 - accuracy: 0.9989 - val_loss: 0.2011 - val_accuracy: 0.9848\n",
      "Epoch 9/20\n",
      "37/36 [==============================] - 20s 544ms/step - loss: 0.0194 - accuracy: 0.9983 - val_loss: 0.1256 - val_accuracy: 0.9899\n",
      "Epoch 10/20\n",
      "37/36 [==============================] - 20s 550ms/step - loss: 0.0166 - accuracy: 0.9982 - val_loss: 0.0788 - val_accuracy: 0.9949\n",
      "Epoch 11/20\n",
      "37/36 [==============================] - 20s 547ms/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 0.0981 - val_accuracy: 0.9929\n",
      "Epoch 12/20\n",
      "37/36 [==============================] - 20s 548ms/step - loss: 0.0135 - accuracy: 0.9982 - val_loss: 0.1108 - val_accuracy: 0.9929\n",
      "Epoch 13/20\n",
      "37/36 [==============================] - 20s 543ms/step - loss: 0.0134 - accuracy: 0.9986 - val_loss: 0.1503 - val_accuracy: 0.9899\n",
      "Epoch 14/20\n",
      "37/36 [==============================] - 20s 544ms/step - loss: 0.0120 - accuracy: 0.9989 - val_loss: 0.0414 - val_accuracy: 0.9959\n",
      "Epoch 15/20\n",
      "37/36 [==============================] - 20s 552ms/step - loss: 0.0145 - accuracy: 0.9986 - val_loss: 0.1487 - val_accuracy: 0.9878\n",
      "Epoch 16/20\n",
      "37/36 [==============================] - 20s 552ms/step - loss: 0.0130 - accuracy: 0.9986 - val_loss: 0.1269 - val_accuracy: 0.9899\n",
      "Epoch 17/20\n",
      "37/36 [==============================] - 20s 548ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.1923 - val_accuracy: 0.9858\n",
      "Epoch 18/20\n",
      "37/36 [==============================] - 20s 546ms/step - loss: 0.0126 - accuracy: 0.9986 - val_loss: 0.1612 - val_accuracy: 0.9889\n",
      "Epoch 19/20\n",
      "37/36 [==============================] - 20s 550ms/step - loss: 0.0074 - accuracy: 0.9994 - val_loss: 0.0959 - val_accuracy: 0.9904\n",
      "Epoch 20/20\n",
      "37/36 [==============================] - 20s 552ms/step - loss: 0.0082 - accuracy: 0.9992 - val_loss: 0.1179 - val_accuracy: 0.9868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6ada5e5e80>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(FIXME,\n",
    "#           validation_data=FIXME,\n",
    "#           steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "#           validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "#           epochs=FIXME)\n",
    "\n",
    "model.fit(train_it, \n",
    "          validation_data=valid_it, \n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解冻模型以进行微调\n",
    "如果您已经达到了92％的验证准确率，则此步是可选的。如果没有，我们建议您以很小的学习率尝试对模型进行微调。您可以再次使用笔记本05b作为参考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "# base_model.trainable = FIXME\n",
    "base_model.trainable = True\n",
    "\n",
    "# Compile the model with a low learning rate\n",
    "# model.compile(optimizer=keras.optimizers.RMSprop(learning_rate = FIXME),\n",
    "#               loss = FIXME , metrics = FIXME)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate = .00001),  # Very low learning rate\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "148/147 [==============================] - 30s 204ms/step - loss: -1.2942 - binary_accuracy: 0.1413 - val_loss: -1.2703 - val_binary_accuracy: 0.1672\n",
      "Epoch 2/20\n",
      "148/147 [==============================] - 24s 162ms/step - loss: -1.3128 - binary_accuracy: 0.1421 - val_loss: -1.3190 - val_binary_accuracy: 0.1550\n",
      "Epoch 3/20\n",
      "148/147 [==============================] - 24s 162ms/step - loss: -1.2942 - binary_accuracy: 0.1438 - val_loss: -1.2308 - val_binary_accuracy: 0.1429\n",
      "Epoch 4/20\n",
      "148/147 [==============================] - 24s 162ms/step - loss: -1.2993 - binary_accuracy: 0.1438 - val_loss: -1.3038 - val_binary_accuracy: 0.1398\n",
      "Epoch 5/20\n",
      "148/147 [==============================] - 24s 163ms/step - loss: -1.3043 - binary_accuracy: 0.1421 - val_loss: -1.3433 - val_binary_accuracy: 0.1459\n",
      "Epoch 6/20\n",
      "148/147 [==============================] - 24s 163ms/step - loss: -1.3103 - binary_accuracy: 0.1430 - val_loss: -1.2734 - val_binary_accuracy: 0.1429\n",
      "Epoch 7/20\n",
      "148/147 [==============================] - 24s 162ms/step - loss: -1.3035 - binary_accuracy: 0.1421 - val_loss: -1.2825 - val_binary_accuracy: 0.1520\n",
      "Epoch 8/20\n",
      "148/147 [==============================] - 24s 163ms/step - loss: -1.2967 - binary_accuracy: 0.1438 - val_loss: -1.2582 - val_binary_accuracy: 0.1611\n",
      "Epoch 9/20\n",
      "148/147 [==============================] - 24s 163ms/step - loss: -1.2984 - binary_accuracy: 0.1430 - val_loss: -1.2946 - val_binary_accuracy: 0.1277\n",
      "Epoch 10/20\n",
      "148/147 [==============================] - 24s 163ms/step - loss: -1.3026 - binary_accuracy: 0.1430 - val_loss: -1.3767 - val_binary_accuracy: 0.1459\n",
      "Epoch 11/20\n",
      "148/147 [==============================] - 24s 163ms/step - loss: -1.2942 - binary_accuracy: 0.1430 - val_loss: -1.2369 - val_binary_accuracy: 0.1550\n",
      "Epoch 12/20\n",
      "148/147 [==============================] - 24s 162ms/step - loss: -1.3111 - binary_accuracy: 0.1430 - val_loss: -1.1609 - val_binary_accuracy: 0.1611\n",
      "Epoch 13/20\n",
      "148/147 [==============================] - 24s 163ms/step - loss: -1.3103 - binary_accuracy: 0.1421 - val_loss: -1.4071 - val_binary_accuracy: 0.1277\n",
      "Epoch 14/20\n",
      "148/147 [==============================] - 24s 163ms/step - loss: -1.2976 - binary_accuracy: 0.1430 - val_loss: -1.2247 - val_binary_accuracy: 0.1641\n",
      "Epoch 15/20\n",
      "148/147 [==============================] - 24s 163ms/step - loss: -1.2967 - binary_accuracy: 0.1447 - val_loss: -1.3129 - val_binary_accuracy: 0.1520\n",
      "Epoch 16/20\n",
      "148/147 [==============================] - 24s 163ms/step - loss: -1.2984 - binary_accuracy: 0.1421 - val_loss: -1.3311 - val_binary_accuracy: 0.1368\n",
      "Epoch 17/20\n",
      "148/147 [==============================] - 24s 163ms/step - loss: -1.3060 - binary_accuracy: 0.1438 - val_loss: -1.2855 - val_binary_accuracy: 0.1550\n",
      "Epoch 18/20\n",
      "148/147 [==============================] - 24s 163ms/step - loss: -1.2984 - binary_accuracy: 0.1430 - val_loss: -1.2551 - val_binary_accuracy: 0.1429\n",
      "Epoch 19/20\n",
      "148/147 [==============================] - 24s 163ms/step - loss: -1.2984 - binary_accuracy: 0.1447 - val_loss: -1.2825 - val_binary_accuracy: 0.1581\n",
      "Epoch 20/20\n",
      "148/147 [==============================] - 24s 164ms/step - loss: -1.3119 - binary_accuracy: 0.1404 - val_loss: -1.3038 - val_binary_accuracy: 0.1489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6ad00c3cc0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(FIXME,\n",
    "#           validation_data=FIXME,\n",
    "#           steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "#           validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "#           epochs=FIXME)\n",
    "\n",
    "model.fit(train_it, \n",
    "          validation_data=valid_it, \n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "希望您现在拥有的模型具有92％或更高的验证准确率。如果没有，您可能需要返回并对模型进行更多的训练，或者对数据增强进行调整。\n",
    "\n",
    "对验证精度满意后，您可以通过执行以下单元格来评估模型。`evaluate`函数将返回一个元组（tuple），其中第一个值是您的损失，第二个值是您的准确率。您需要获得0.92或更高的精度值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [================================] - 4s 355ms/step - loss: 0.0467 - accuracy: 0.9939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.046691492199897766, 0.9939209818840027]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_it, steps=valid_it.samples/valid_it.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 执行评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请执行以下2个代码单元来评估您的结果。\n",
    "\n",
    "**注意：** `run_assessment` 假设您的模型是以 `model` 命名的，而且您的测试数据集的名字是`valid_it`。无论出于什么原因您修改了上述名字，请在下面的单元中对`run_assessment`的参数做相应的修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_assessment import run_assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model 5 times to obtain average accuracy...\n",
      "\n",
      "11/10 [================================] - 4s 389ms/step - loss: 0.1033 - accuracy: 0.9919\n",
      "11/10 [================================] - 4s 353ms/step - loss: 0.1153 - accuracy: 0.9889\n",
      "11/10 [================================] - 4s 363ms/step - loss: 0.1012 - accuracy: 0.9924\n",
      "11/10 [================================] - 4s 358ms/step - loss: 0.1245 - accuracy: 0.9899\n",
      "11/10 [================================] - 4s 359ms/step - loss: 0.1014 - accuracy: 0.9919\n",
      "\n",
      "Accuracy required to pass the assessment is 0.92 or greater.\n",
      "Your average accuracy is 0.9910.\n",
      "\n",
      "Congratulations! You passed the assessment!\n",
      "See instructions below to generate a certificate.\n"
     ]
    }
   ],
   "source": [
    "run_assessment(model, valid_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成证书"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您通过了评估，请返回课程页面（见下图）并单击Assess（评估）按钮，就会产生本课程的合格证书。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/assess_task.png\" style=\"width: 800px;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
